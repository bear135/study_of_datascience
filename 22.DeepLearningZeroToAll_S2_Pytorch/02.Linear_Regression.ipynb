{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 02. Linear Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Summary : Full Code for Linear Regression\n",
    "    step 1 - training data 정의 \n",
    "    step 2 - Hypothesis 정의&초기화 \n",
    "    step 3 - optimizer 정의 \n",
    "    step 4 - 학습 반복 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- Optimizer = SGD(Stochastic Gradient Boost) \n",
    "    - torch.optim 라이브러리를 사용하며, 다음 3가지가 항상 사용됨 \n",
    "        - 1) zero_grad() : gradient 초기화 \n",
    "        - 2) backward() : gradient 계산 \n",
    "        - 3) step() : 점진 개선 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 | W: 0.187 | b: 0.080 | Cost: 18.667\nEpoch  100/1000 | W: 1.746 | b: 0.578 | Cost: 0.048\nEpoch  200/1000 | W: 1.800 | b: 0.454 | Cost: 0.030\nEpoch  300/1000 | W: 1.843 | b: 0.357 | Cost: 0.018\nEpoch  400/1000 | W: 1.876 | b: 0.281 | Cost: 0.011\nEpoch  500/1000 | W: 1.903 | b: 0.221 | Cost: 0.007\nEpoch  600/1000 | W: 1.924 | b: 0.174 | Cost: 0.004\nEpoch  700/1000 | W: 1.940 | b: 0.136 | Cost: 0.003\nEpoch  800/1000 | W: 1.953 | b: 0.107 | Cost: 0.002\nEpoch  900/1000 | W: 1.963 | b: 0.084 | Cost: 0.001\nEpoch 1000/1000 | W: 1.971 | b: 0.066 | Cost: 0.001\n"
     ]
    }
   ],
   "source": [
    "## library import \n",
    "import torch \n",
    "import torch.optim as optim\n",
    "\n",
    "### set the training datasets \n",
    "x_train = torch.FloatTensor([\n",
    "    [1], [2], [3]\n",
    "])\n",
    "y_train = torch.FloatTensor([\n",
    "    [2], [4], [6]\n",
    "])\n",
    "\n",
    "### set and initialize the W, b (as 0)\n",
    "W = torch.zeros(1, requires_grad=True) \n",
    "b = torch.zeros(1, requires_grad=True) \n",
    "\n",
    "### set the training method\n",
    "### W와 b를 SGD & learning rate = 0.01로 학습시킨다. \n",
    "optimizer = optim.SGD([W,b], lr = 0.01)\n",
    "\n",
    "### training 반복횟수 지정 \n",
    "nb_epochs = 1000 \n",
    "for epoch in range(nb_epochs + 1): \n",
    "    hypothesis = x_train*W + b \n",
    "    cost = torch.mean((hypothesis - y_train)**2) # cost function = MSE \n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    cost.backward()\n",
    "    optimizer.step() \n",
    "\n",
    "    ### print the result (100번째 마다 출력하라)\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} | W: {:.3f} | b: {:.3f} | Cost: {:.3f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
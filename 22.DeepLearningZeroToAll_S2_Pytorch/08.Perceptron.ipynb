{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "scientific-accommodation",
   "metadata": {},
   "source": [
    "### 08. Perception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-trick",
   "metadata": {},
   "source": [
    "- and, or 문제는 선형 구분이 가능하다 (perceptron availalbe) \n",
    "- 반면 xor 문제는 하나의 perceptron으로 구분 불가, 다수의 perceptron을 사용해야 한다. \n",
    "- 다수의 perceptron을 사용할 수 있는 방법으로써 Backpropagation(역전파 알고리즘)이 개발되었다. \n",
    "    - target값과 실제 모델이 계산한 output이 얼마나 차이가 나는지 구한 후, 그 오차값을 다시 뒤로 전파해가면서 각 노드가 가지고 있는 변수들을 갱신하는 알고리즘 (즉, 출력부터 반대 방향으로 순차 편미분 수행하여, W와 b값들을 갱신)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-instrument",
   "metadata": {},
   "source": [
    "- <font color = 'yellow'> #01 선형모델 2개를 사용하여 xor 문제를 해결하는 코드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "obvious-plate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  cost:0.6945\n",
      "epoch: 1000  cost:0.6169\n",
      "epoch: 2000  cost:0.0111\n",
      "epoch: 3000  cost:0.0051\n",
      "epoch: 4000  cost:0.0033\n",
      "epoch: 5000  cost:0.0024\n",
      "epoch: 6000  cost:0.0019\n",
      "epoch: 7000  cost:0.0016\n",
      "epoch: 8000  cost:0.0014\n",
      "epoch: 9000  cost:0.0012\n",
      "epoch: 10000  cost:0.0010\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "device = 'cpu' \n",
    "\n",
    "## xor data \n",
    "X = torch.FloatTensor([\n",
    "    [0,0], [0,1], [1,0], [1,1]\n",
    "]).to(device)\n",
    "\n",
    "Y = torch.FloatTensor([\n",
    "    [0], [1], [1], [0]\n",
    "])\n",
    "\n",
    "## models, function(Sigmoid), loss function, optimizer \n",
    "linear1 = nn.Linear(2,2, bias = True)\n",
    "linear2 = nn.Linear(2,1, bias = True)\n",
    "sigmoid = nn.Sigmoid()\n",
    "model = nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device) ## Sequential: 모델들을 순차적으로 담아 수행시켜 준다. \n",
    "\n",
    "criterion = nn.BCELoss().to(device)  ## 0, 1로 구분하는 문제이므로, crossEntropyLoss 대신 BCELoss를 사용 \n",
    "optimizer = optim.SGD(model.parameters(), lr = 1)\n",
    "\n",
    "for epoch in range(10001): \n",
    "    hypothesis = model(X)\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0 : \n",
    "        print('epoch: {}  cost:{:.4f}'.format(epoch, cost.item())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "touched-beast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis:  tensor([[1.0020e-03],\n",
      "        [9.9862e-01],\n",
      "        [9.9907e-01],\n",
      "        [8.7177e-04]]) \n",
      "Accuracy:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('Hypothesis: ', hypothesis, '\\nAccuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-socket",
   "metadata": {},
   "source": [
    "- <font color = 'yellow'> #02. 같은 방식으로 선형모형을 4개로 확장한 코드 \n",
    "    - 앞서 대비 cost의 감소 주목 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "informational-preview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  cost:0.6944\n",
      "epoch: 1000  cost:0.6931\n",
      "epoch: 2000  cost:0.6930\n",
      "epoch: 3000  cost:0.6922\n",
      "epoch: 4000  cost:0.0049\n",
      "epoch: 5000  cost:0.0008\n",
      "epoch: 6000  cost:0.0004\n",
      "epoch: 7000  cost:0.0003\n",
      "epoch: 8000  cost:0.0002\n",
      "epoch: 9000  cost:0.0002\n",
      "epoch: 10000  cost:0.0001\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "device = 'cpu' \n",
    "\n",
    "## xor data \n",
    "X = torch.FloatTensor([\n",
    "    [0,0], [0,1], [1,0], [1,1]\n",
    "]).to(device)\n",
    "\n",
    "Y = torch.FloatTensor([\n",
    "    [0], [1], [1], [0]\n",
    "])\n",
    "\n",
    "## models, function(Sigmoid), loss function, optimizer \n",
    "linear1 = nn.Linear(2,10, bias = True)\n",
    "linear2 = nn.Linear(10,10, bias = True)\n",
    "linear3 = nn.Linear(10,10, bias = True)\n",
    "linear4 = nn.Linear(10,1, bias = True)\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "model = nn.Sequential(linear1, sigmoid, \n",
    "                      linear2, sigmoid, \n",
    "                      linear3, sigmoid, \n",
    "                      linear4, sigmoid, \n",
    "                     ).to(device) ## Sequential: 모델들을 순차적으로 담아 수행시켜 준다. \n",
    "\n",
    "criterion = nn.BCELoss().to(device)  ## 0, 1로 구분하는 문제이므로, crossEntropyLoss 대신 BCELoss를 사용 \n",
    "optimizer = optim.SGD(model.parameters(), lr = 1)\n",
    "\n",
    "for epoch in range(10001): \n",
    "    hypothesis = model(X)\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0 : \n",
    "        print('epoch: {}  cost:{:.4f}'.format(epoch, cost.item())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "focal-anderson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis:  tensor([[1.1590e-04],\n",
      "        [9.9988e-01],\n",
      "        [9.9987e-01],\n",
      "        [1.3150e-04]]) \n",
      "Accuracy:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('Hypothesis: ', hypothesis, '\\nAccuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-diana",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

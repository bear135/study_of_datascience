{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 12: RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN은 sequence data를 처리하는 모델이다. sequence는 순서대로 처리해야 하는 것을 뜻하고, 이런 데이터에는 음성인식, 자연어처리 등이 포함된다. \n",
    "- 자연어의 경우 단어 하나만 안다고 해서 처리될 수 없고, 앞뒤 문맥을 함께 이해해야 해석이 가능하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 연속적인 값들을 다루기 때문에 state라는 개념이 들어간다. \n",
    "- 이전의(old state) 값이 현재(new state) 값에 영향을 미친다. \n",
    "- 하나의 함수와 동일한 parameters가 모든 states에 동일하게 적용된다. \n",
    "- t는 sequence data에 대한 특정 시점의 데이터를 가리키고, 여기서는 t에 대해 두 가지를 계산한다. \n",
    "- 첫 번째 줄의 공식은 W의 이전 상태와 입력을 갖고 fw에 해당하는 tanh 함수를 호출하는 것이다. \n",
    "- ht는 현재 상태를 의미하고 h의 t-1번째는 이전(old) 상태와 입력 값(x)을 사용해서 계산한다. \n",
    "- yt는 예측 값을 의미하고, W와 현재 상태(ht)를 곱해서 계산한다.\n",
    "\n",
    "<left><img src= 'img/rnn_1.PNG' width=\"70%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color = 'yellow'> Ex. 4가지 종류의 글자 h, e, l, o가 있고, 연속된 sequence인 \"hello\"에 대해 이후에 나올 값을 예측하려함 </font>\n",
    "\n",
    "- 4가지 종류의 글자가 있기 때문에 크기가 4인 벡터(리스트)로 처리\n",
    "- h의 값과 x의 값을 W와 계산한 다음 tanh 함수에 전달하면 hidden layer에서 보여주는 값이 차례대로 생성됨 \n",
    "- 매번 계산이 끝날 때마다 새로운 상태를 가리키는 hidden layer의 값이 변화함  \n",
    "- tanh 함수는 sigmoid 함수 중의 하나로 처음 나왔던 sigmoid를 개량한 버전 \n",
    "- 기존의 sigmoid가 0에서 1 사이의 값을 반환하는데, tanh 함수는 -1에서 1 사이의 값을 반환하도록 개량. 현재 상태를 가리키는 ht는 tanh 함수의 반환값이므로 -1과 1 사이의 값이 된며, 따라서 hidden layer에 있는 값들도 해당 범위에 존재하게 됨 \n",
    "- 최종적으로는 모든 단계에서 값을 예측하고 실제 값과 맞는지 비교\n",
    "\n",
    "<left><img src= 'img/rnn_2.PNG' width=\"70%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color = 'yellow'> RNN의 주요 활용분야 </font>\n",
    "- 그림1. 바닐라 RNN을 가리킨다. 가장 단순한 형태로 1대1(one-to-one) 기반의 모델\n",
    "\n",
    "- 그림2. 1대다(one-to-many) 기반의 모델로 이미지에 대해 설명을 붙일 때 사용한다.\n",
    "\n",
    "  *ex. 한 장의 그림에 대해 \"소년이 사과를 고르고 있다\"는 자막을 붙일때*\n",
    "\n",
    "- 그림3. 다대1(many-to-one) 형태의 모델로 여러 개의 입력에 대해 하나의 결과를 만들어 준다. \n",
    "\n",
    "  *ex. 우리가 하는 말을 통해 우리의 심리 상태를 \"안정\", \"불안\", \"공포\" 등의 한 단어로 결과를 예측*\n",
    "\n",
    "- 그림4. 다대다(many-to-many) 형태의 모델로 기계 번역에서 사용\n",
    "\n",
    "- 그림5. 다대다(many-to-many) 모델의 또 다른 형태.\n",
    "  \n",
    "  *ex. 동영상 같은 경우는 여러 개의 이미지 프레임에 대해 여러 개의 설명이나 번역 형태로 결과를 반환* \n",
    "\n",
    "<left><img src= 'img/rnn_3.PNG' width=\"70%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 12-1: RNN Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN 모형의 구성은 cell과 output으로 이루어 진다. \n",
    "- cell은 적용할 로직(BasicRNN 또는 Basic LSTM 등)과 hidden layer 갯수를 결정 \n",
    "- output은 데이터와 cell 값을 입력받아, 출력값과 states값을 반환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><img src= 'img/one_cell.PNG' width=\"30%\"></left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "array([[[1., 0., 0., 0.]]], dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-3-e0f526a47d3e>:9: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "array([[[-0.22140579,  0.32247472]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('one_cell') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2)\n",
    "    hidden_size = 2            ### 레이어가 2개이므로, 2개의 결과값을 반환할 것이다. \n",
    "    cell = tf.keras.layers.SimpleRNNCell(units=hidden_size)\n",
    "    print(cell.output_size, cell.state_size)\n",
    "\n",
    "    x_data = np.array([[[1,0,0,0]]], dtype=np.float32) \n",
    "    pp.pprint(x_data)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><img src= 'img/swquence5.PNG' width=\"70%\"></left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 4)\n",
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]]], dtype=float32)\n",
      "array([[[ 0.23053104,  0.6791696 ],\n",
      "        [-0.16352612,  0.3833307 ],\n",
      "        [ 0.7607948 , -0.65091115],\n",
      "        [-0.22333062, -0.12599911],\n",
      "        [ 0.34676367, -0.0395067 ]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding for each char in 'hello'\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "with tf.variable_scope('two_sequances') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
    "    hidden_size = 2\n",
    "    cell = tf.keras.layers.SimpleRNNCell(units=hidden_size)\n",
    "    x_data = np.array([[h, e, l, l, o]], dtype=np.float32)\n",
    "    print(x_data.shape)\n",
    "    pp.pprint(x_data)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch를 사용하여 많은 데이터를 한번에 입력 \n",
    "\n",
    "<left><img src= 'img/batch.PNG' width=\"70%\"></left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]],\n",
      "\n",
      "       [[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]],\n",
      "\n",
      "       [[0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]]], dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-6-f7360e7b5af0>:10: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "array([[[-0.0494266 ,  0.13403285],\n",
      "        [-0.03541628, -0.06928813],\n",
      "        [-0.16191316, -0.07597242],\n",
      "        [-0.2210239 , -0.1117771 ],\n",
      "        [-0.21533628, -0.07105639]],\n",
      "\n",
      "       [[-0.01348959, -0.1945803 ],\n",
      "        [-0.12799458, -0.05386541],\n",
      "        [-0.21048583, -0.09292484],\n",
      "        [-0.24717641, -0.12732199],\n",
      "        [-0.2697491 , -0.15393722]],\n",
      "\n",
      "       [[-0.12936343, -0.04072882],\n",
      "        [-0.20017329, -0.08317643],\n",
      "        [-0.14473794, -0.33470598],\n",
      "        [-0.15610597, -0.448994  ],\n",
      "        [-0.2917408 , -0.23788437]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('3_batches') as scope:\n",
    "    # One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
    "    # 3 batches 'hello', 'eolll', 'lleel'\n",
    "    x_data = np.array([[h, e, l, l, o],\n",
    "                       [e, o, l, l, l],\n",
    "                       [l, l, e, e, l]], dtype=np.float32)\n",
    "    pp.pprint(x_data)\n",
    "    \n",
    "    hidden_size = 2\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 12-2: RNN ~ Hi Hello training \n",
    "- 입력된 문자의 다음에 어떤 문자가 나올지를 예측하는 문제 \n",
    "- hidden_size = 5, input_dim = 5 ~ hihello에서 유니크한 문자 갯수는 5개, 즉 예측해야 하는 값 또한 5개의 문자 \n",
    "- input_dim = 5 ~ hihello에서 유니크한 문자 갯수는 5개, 이것을 one-hot 코딩으로 변환하면 5개의 컬럼을 가진다.  \n",
    "- sequence_length = 6 ~ 6개의 문자를 예측/출력해야 한다. \n",
    "- batch_size = 1 ~ 한개의 문자열을 다룬다. \n",
    "\n",
    "<left><img src= 'img/hihello.PNG' width=\"70%\"></left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-7-1fcb35801f83>:27: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\layers\\python\\layers\\layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "#### 문자열 hihello를 데이터로 변환한다. \n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "# Teach hello: hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "y_data = [[1, 0, 2, 3, 3, 4]]    # ihello : 최종적으로 얻어야 하는 값 \n",
    "\n",
    "\n",
    "#### hyper parameters \n",
    "num_classes = 5\n",
    "input_dim = 5          # one-hot size\n",
    "hidden_size = 5        # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1         # one sentence\n",
    "sequence_length = 6    # |ihello| == 6\n",
    "learning_rate = 0.1\n",
    "\n",
    "#### 변수할당, cell과 output 정의, 초기값은 0 \n",
    "X = tf.placeholder(tf.float32, [None, sequence_length, input_dim])  # X one-hot\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "# fc_w = tf.get_variable(\"fc_w\", [hidden_size, num_classes])\n",
    "# fc_b = tf.get_variable(\"fc_b\", [num_classes])\n",
    "# outputs = tf.matmul(X_for_fc, fc_w) + fc_b\n",
    "outputs = tf.contrib.layers.fully_connected(\n",
    "    inputs=X_for_fc, num_outputs=num_classes, activation_fn=None)\n",
    "\n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=weights)  ## sequence_loss함수로 cost를 간단히 계산가능 \n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.674243 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "1 loss: 1.5711254 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "2 loss: 1.5051117 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "3 loss: 1.448404 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "4 loss: 1.3850368 prediction:  [[3 3 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  lllllo\n",
      "5 loss: 1.3141826 prediction:  [[3 3 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  lllllo\n",
      "6 loss: 1.2213479 prediction:  [[3 3 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  lllllo\n",
      "7 loss: 1.1139129 prediction:  [[3 3 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  lllllo\n",
      "8 loss: 1.0035089 prediction:  [[3 3 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  lllllo\n",
      "9 loss: 0.8894078 prediction:  [[3 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  lhello\n",
      "10 loss: 0.7840233 prediction:  [[3 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  lhello\n",
      "11 loss: 0.68109804 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "12 loss: 0.5838521 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "13 loss: 0.49519873 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "14 loss: 0.41397536 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "15 loss: 0.3401749 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "16 loss: 0.27501115 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "17 loss: 0.21899776 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "18 loss: 0.17167449 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "19 loss: 0.13404457 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "20 loss: 0.10527653 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "21 loss: 0.08319207 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "22 loss: 0.06711161 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "23 loss: 0.054903213 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "24 loss: 0.04545326 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "25 loss: 0.038518626 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "26 loss: 0.03274915 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "27 loss: 0.028219258 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "28 loss: 0.024659531 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "29 loss: 0.021497985 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "30 loss: 0.01901337 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "31 loss: 0.016970338 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "32 loss: 0.015114781 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "33 loss: 0.013651361 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "34 loss: 0.012391515 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "35 loss: 0.011215358 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "36 loss: 0.010285571 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "37 loss: 0.009473552 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "38 loss: 0.008691863 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "39 loss: 0.008082625 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "40 loss: 0.007527467 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "41 loss: 0.0070049004 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "42 loss: 0.0066070594 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "43 loss: 0.006210523 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "44 loss: 0.00585675 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "45 loss: 0.0055781156 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "46 loss: 0.005286314 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "47 loss: 0.005050713 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "48 loss: 0.004840816 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "49 loss: 0.004624869 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n"
     ]
    }
   ],
   "source": [
    "#### running \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_one_hot})\n",
    "        print(i, \"loss:\", l, \"prediction: \", result, \"true Y: \", y_data)\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        print(\"\\tPrediction str: \", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

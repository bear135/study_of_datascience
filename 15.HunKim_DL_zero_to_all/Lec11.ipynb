{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 11-1: CNN introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolutional Neural Network\n",
    "<left><img src= 'img/cnn0.PNG' width=\"70%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow' > CNN의 기본원리 </font>\n",
    "- 크기가 32 x32 pixels 이고 depth 즉 color가 3인, 32x32x3의 이미지에 대해 \n",
    "- 임의의 크기, 예를 들어 5x5x3의 filter(weight)를 적용하여 하나의 값을 얻어낸다. \n",
    "- 이 과정을 반복하여 전체에 대한 값을 얻는다. \n",
    "\n",
    "<left><img src= 'img/cnn1.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filter를 지정된 칸수(stride)만큼 계속 이동하면서, 전체 이미지에 대한 값들을 수집한다. \n",
    "- 이때 결과물, 즉 출력 크기 = (입력 크기 - 필터 크기) / stride 크기 +1\n",
    "\n",
    "<left><img src= 'img/stride.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그런데 convolutinal layer를 거칠 때마다 크기가 작아지는 문제가 발생한다. \n",
    "- stride 크기에 상관없이 최소한 (필터 크기-1)만큼 줄어들게 되는데, 원본 이미지의 크기가 줄어들지 않도록 padding을 넣는다. \n",
    "- 보통은 stride를 1로 하기 때문에 padding의 크기 또한 1이 된며, 테두리 전체에 대해 추가되기 때문에 크기는 2만큼 증가한다.\n",
    "\n",
    "<left><img src= 'img/padding.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 그림에서 원본의 사이즈는 32, 필터의 크기는 5, stride = 1 이므로 \n",
    "- 출력결과(activation map)ation map)은, (32-5)/1+1 = 28 이다. (no padding)\n",
    "- 6개의 필터가 사용되었으므로, total activation maps = (28,28,6) \n",
    "\n",
    "<left><img src= 'img/cnn2.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 11-2: CNN의 핵심 - Pooling, max pooling, fully connected "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pooling은 sampling, resizing과 같은 의미이다. \n",
    "- 아래 그림에서 4x4의 원본에 대해 2x2의 필터가 stride = 2로 주어졌으므로, output size = 2x2이다. \n",
    "- 이때 예를들어 첫번째 필터링 부분, [1,1,5,6]을 하나의 값으로 바꿔줘야 하는데, 흔히 max값을 사용한다. (max pooling)\n",
    "\n",
    "<left><img src= 'img/pooling.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN의 특징은 처음부터 끝까지 layer들이 하나로 연결되어 있다는 것이다. \n",
    "- 이것을 Fully Connected 라고 한다. \n",
    "\n",
    "<left><img src= 'img/cnn0.PNG' width=\"60%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question. 왜 Max Pooling을 사용할까?\n",
    "- mean, median, min, max 등을 고려할 수 있으나, \n",
    "- 평균은 필터 영역의 특징을 표현하는데 부적합. 평균을 사용하면 여러 번의 pooling을 거치면 전체 값들이 비슷해지는 증상을 보일것이다. (평균의 평균의 평균? 결국 모든 그림이 다 똑같은 그림이라는 결론 도출?)  \n",
    "- min을 사용한다면 LeRU를 거치면서 음수는 0이 되어 버린다. \n",
    "- 실제로 자연계에서도 우리는 보이는 모든 것을 기억하는 것이 아니라 두드러진 특징 몇 가지를 기억하는 것임. CNN에서 두드러진 특징을 찾기 위한 방법이 max pooling이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 11-3: CNN 발전과정과 사례들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 1. LeNet </font>\n",
    "- 1998년 Prof. LeCun에 의해 만들어진 고전 모델, 6개의 hidden layer 사용 \n",
    "1. Input - 크기 32x32x1. 흑백 이미지. (필터 5x5, stride 1)\n",
    "2. C1 - 크기 28x28x6. feature maps 6개. (필터 2x2, subsampling)\n",
    "3. S2 - 크기 14x14x6. feature maps 6개. (필터 5x5, stride 1)\n",
    "4. C3 - 크기 10x10x16. feature maps 16개\n",
    "5. S4 - 크기 5x5x16. feature maps 16개.\n",
    "6. C5 - FC(Full Connection )120개\n",
    "7. F6 - FC(Full Connection) 84개\n",
    "8. Output - GC(Gaussian Connections) 10개. (최종 분류)\n",
    "\n",
    "<left><img src= 'img/lanet.PNG' width=\"80%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 2. AlexNet </font>\n",
    "- 2012년에 Prof.Hint의 lab에서 발표 \n",
    "- 227x227x3의 원본이미지에 대해, Relu/Dropout/Ensemble을 동원하여 정확도를 높임  \n",
    "\n",
    "<left><img src= 'img/alexnet.PNG' width=\"80%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 3. GoogLeNet </font>\n",
    "- 2014년 Google이 발표, inception module이라는 새로운 이론을 적용\n",
    "- AlexNet에 비해 Deep 하나 Wide 하지는 않으며, \n",
    "- CNN에서 보통 마지막에만 두는 FC가 중간에 여러개 들어가는 것이 특징 \n",
    "\n",
    "<left><img src= 'img/googlenet.PNG' width=\"80%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 4. ResNet </font>\n",
    "- 2015년 He가 발표, AlexNet이 사용했던 layer가 8개였던데 비해 무려 152개 layer를 사용\n",
    "- 단 Fast forward 방식과 유사한 Fast Net을 적용하여 연산속도를 빠르게 구현했다. \n",
    "\n",
    "<left><img src= 'img/resnet.PNG' width=\"80%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 참고. ResNet vs. GoogLeNet </font>\n",
    "- ResNet과 GoogLeNet의 공통점을 보여주고 있다. 모양은 다르지만, 목표까지 가기 위해 거치는 layer의 갯수가 적다. \n",
    "- ResNet은 일렬로 배치해서 건너 뛰도록 설계했고, GoogLeNet은 처음부터 길을 여러 개 만들었다.\n",
    "- GoogLeNet의 단점은 경로가 여러 개이긴 하지만, 특정 경로로 진입하면 경로에 포함된 모든 layer를 거쳐야 한다는 점이 아닐까 싶다. \n",
    "- ResNet이 매번 통과할 때마다 거치는 layer의 구성이 달라지는 반면 GoogLeNet은 구성이 같기 때문에 다양한 결과를 만들지 못하는 것이 단점으로 보인다. \n",
    "- ResNet이 GoogLeNet보다 더욱 균형 잡힌 결과를 만들어 낼 수 있는 구조라고 보여진다.\n",
    "\n",
    "<left><img src= 'img/resandgoo.PNG' width=\"80%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 5. CNN for sentence classification </font>\n",
    "- 2014년 한국인 윤킴 교수가 만든 모델\n",
    "- Text, Sentence 분석을 위해 많이 사용된다. \n",
    "\n",
    "<left><img src= 'img/yoonkim.PNG' width=\"80%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 6. DeepMind AlphaGo </font>\n",
    "- 19x19x48 이미지 사용 : 바둑판의 크기가 19x19이며, 돌의 특징을 48종으로 구분 (흑돌인지 백돌인지, 주변에 흑돌이 있는지 백돌이 있는지, 현재 정세가 어떠한지 등등..)\n",
    "- 이후 이미지 크기에 패딩을 적용하여 23x23으로 변형 \n",
    "\n",
    "<left><img src= 'img/nature.PNG' width=\"80%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 11-1: CNN Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "tf.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2102af9c470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 간단한 원본 이미지를 하나 만들어 보자 \n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "image = np.array([[[[1],[2],[3]], \n",
    "                 [[4],[5],[6]],\n",
    "                 [[7],[8],[9]]]], dtype = np.float32 )\n",
    "print(image.shape)\n",
    "plt.imshow(image.reshape(3,3), cmap = 'Greys')\n",
    "## 1개의, 3x3x1인 이미지가 만들어 진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 원리 이해 </font>\n",
    "- 1개의 3x3x1 원본 이미지에 대해, \n",
    "- 2x2x1인 필터를 1개 적용(stride는 1) *필터의 값은 all = 1 \n",
    "- 첫번째 필터결과 1+2+4+5 = 12, 두번째 필터결과 2+3+5+6 = 16, 3rd = 24, 4th = 28\n",
    "\n",
    "<left><img src= 'img/simplecnn.PNG' width=\"60%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이때 padding 옵션을 VALID가 아닌 SAME으로 주면, \n",
    "- 원본이미지 크기와 같은 크기의 출력을 만들기 위해 tf가 자동으로 테두리(0값)을 추가한다. \n",
    "- 예를들어 3번째 필터링된 값은, 3+0+6+0 = 9\n",
    "\n",
    "<left><img src= 'img/paddingsame.PNG' width=\"60%\"></left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.]\n",
      "   [2.]\n",
      "   [3.]]\n",
      "\n",
      "  [[4.]\n",
      "   [5.]\n",
      "   [6.]]\n",
      "\n",
      "  [[7.]\n",
      "   [8.]\n",
      "   [9.]]]]\n"
     ]
    }
   ],
   "source": [
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 1)\n",
      "conv2d_img (1, 2, 2, 1)\n",
      "---------------------------------------\n",
      "[[12. 16.]\n",
      " [24. 28.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### conv2d 함수로 CNN을 쉽게 구현할 수 있다. (padding = 'VALID')\n",
    "print('image.shape', image.shape)\n",
    "\n",
    "weight = tf.constant([[[ [1.]],[[1.] ]],\n",
    "                      [[ [1.]],[[1.] ]]])\n",
    "print('weight.shape', weight.shape)\n",
    "\n",
    "conv2d = tf.nn.conv2d(image, weight, strides = [1,1,1,1], padding = 'VALID')\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img\", conv2d_img.shape)\n",
    "\n",
    "#### 시각화를 위한 부분(option)\n",
    "print('---------------------------------------')\n",
    "\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate (conv2d_img): \n",
    "    print(one_img.reshape(2,2)) \n",
    "    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(2,2), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 1)\n",
      "conv2d_img (1, 3, 3, 1)\n",
      "---------------------------------------\n",
      "[[12. 16.  9.]\n",
      " [24. 28. 15.]\n",
      " [15. 17.  9.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### conv2d 함수로 CNN을 쉽게 구현할 수 있다. (padding = 'SAME')\n",
    "print('image.shape', image.shape)\n",
    "\n",
    "weight = tf.constant([[[ [1.]],[[1.] ]],\n",
    "                      [[ [1.]],[[1.] ]]])\n",
    "print('weight.shape', weight.shape)\n",
    "\n",
    "conv2d = tf.nn.conv2d(image, weight, strides = [1,1,1,1], padding = 'SAME')\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img\", conv2d_img.shape)\n",
    "\n",
    "#### 시각화를 위한 부분(option)\n",
    "print('---------------------------------------')\n",
    "\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate (conv2d_img): \n",
    "    print(one_img.reshape(3,3)) \n",
    "    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 3)\n",
      "conv2d_img (1, 3, 3, 3)\n",
      "---------------------------------------\n",
      "[[12. 16.  9.]\n",
      " [24. 28. 15.]\n",
      " [15. 17.  9.]]\n",
      "[[120. 160.  90.]\n",
      " [240. 280. 150.]\n",
      " [150. 170.  90.]]\n",
      "[[-12. -16.  -9.]\n",
      " [-24. -28. -15.]\n",
      " [-15. -17.  -9.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### 같은 방법으로 filter를 3장 사용하면, 3개의 출력값을 얻게 된다. \n",
    "print('image.shape', image.shape)\n",
    "\n",
    "weight = tf.constant([[[ [1., 10, -1]],[[1., 10, -1] ]],\n",
    "                      [[ [1., 10, -1]],[[1., 10, -1] ]]])\n",
    "print('weight.shape', weight.shape)\n",
    "\n",
    "conv2d = tf.nn.conv2d(image, weight, strides = [1,1,1,1], padding = 'SAME')\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img\", conv2d_img.shape)\n",
    "\n",
    "#### 시각화를 위한 부분(option)\n",
    "print('---------------------------------------')\n",
    "\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate (conv2d_img): \n",
    "    print(one_img.reshape(3,3)) \n",
    "    plt.subplot(1,3,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> Max pooling의 원리 이해 </font>\n",
    "- 2x2의 원본 이미지에 대해, 필터 사이즈 2x2 1개, strides = 1, 동일한 사이즈의 결과물을 얻기 위해 padding = 'SAME' \n",
    "- Max pooling으로 각 필터영역에서 max값들을 추출 \n",
    "- 결과로 4,3,2,1의 2x2 이미지를 얻게되었다. \n",
    "\n",
    "<left><img src= 'img/maxp.PNG' width=\"80%\"></left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-cf63d80c5ac4>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2102e0ae7b8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### MNIST data loading, and check with image \n",
    "#### MNIST image size = 28x28\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "img = mnist.train.images[0].reshape(28,28)\n",
    "plt.imshow(img, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D_3:0\", shape=(1, 14, 14, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "img = img.reshape(-1, 28, 28, 1)  ## -1은 몇개인지 모르니 알아서 하라는 뜻, 즉 28x28x1인 000개의 이미지\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,5], stddev=0.01))  ## 3x3x1의 필터 5개 사용 \n",
    "conv2d = tf.nn.conv2d(img, W1, strides = [1,2,2,1], padding = 'SAME')  ## strides = 2, Zero padding 사용 \n",
    "print(conv2d)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "conv2d_img = conv2d.eval()\n",
    "\n",
    "#### Zero padding을 사용했지만 strides = 2이므로, 결과이미지 크기는 절반이 된다. \n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate (conv2d_img): \n",
    "    plt.subplot(1,5,i+1), plt.imshow(one_img.reshape(14,14), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool:0\", shape=(1, 7, 7, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAABZCAYAAAAXQW5UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAJyUlEQVR4nO3dW2gUaRYH8P8xdm46MSEaL9EsCoosyhoRXyZEFFwVHwZFdCIiPkge1Ke8uKCCPoirILKgeGFZBVFGESLBqLP7YryA4iUzeJtd421HApsoJmhMzIWzD0btSVV/X/Wlur9M/38wGPvUVJ352zl2ar6qElUFERG5a0SmGyAiIjMOaiIix3FQExE5joOaiMhxHNRERI7joCYictzIIBuJyFIAfwOQA+DvqvpX0/alpaVaUVGRgvbc1dLSgvfv3/8HATPJzc3VwsLC9DSXIX19ffjw4cMAgBcIkImIZMva0E4A/0OA9woz8Zctuaiq+L1uHdQikgPgEIDFAF4BuC0iDar6KNa/U1FRgaampkR7dd7AwAAmTJgAAMsQMJPCwkJUVVWlq8W0U1VcuXIFAB4BmIcAmWSRfMTxXskSzCQOQU59zAfQoqrPVLUXwA8Avgu3LbfdvXsXeXl5YCZfdXR0YPAnhl5m4vGR7xUPZhKHIIO6HMCvUb9/Nfha1mptbUUkEol+Kesz6enpQUFBQfRLWZ9JlN6or5nLJ8wkDkHOUfudM/GcLxKRWgC1ADBlypQk23JbjMvujZkMGWLZwphJFvtNLswEAN8rRkE+Ub8CED15JwNoHbqRqh5T1XmqOq+0tDRV/TmpvLwcfX190S9ZM8nNzU1bf5mQn5+P7u7u6JesmaStucyL/sP35MJM+F6xCTKobwOYLiJTRSQXwPcAGsJty21z587Fx48fwUy+GjNmDLq6ugAgl5l45PO94sFM4mAd1KraD2ALgB8BPAZwVlUfht2Yy0aOHInJkycDzOSLESNGYNasWQAwA8xkqP+C75WhmEkcAq2jVtWLAC6G3MuwUlRUBFWdEXT7SCSCSZMmxay3tnp+8ht2ysrKAOBB0B9VI5EIxo4dG7NeXV1t3ceMGeY/gufPnydVv3HjhrWHADqDZlJSUoIlS5bErB88eDDpZmyn4bZu3WqsHz58OOkeEEcmwKeeTd8/K1assO5j4sSJxnpRUZGxfu3aNWP91KlT1h4SxSsTiYgcx0FNROQ4DmoiIsdxUBMROY6DmojIcRzURESO46AmInIcBzURkeMCXfASr/7+frS3t8es796927qPjRs3JtXD3r17jfWTJ08mtf94FRcXGxflb9q0yboP28MYli1bZqxfuHDBWC8uLrb2kEoVFRXYv39/zPqcOXOs+2hrazPWRXzvw/7F4BWmMU2bNs3aw5B7nCRl/PjxqKuri1nv7Oy07uPAgQPGui3XPXv2GOv379+39nD9+nXrNvEoKSnB6tWrY9ZHjx5t3cfLly+N9Xfv3hnrhw4dMtafPn1q7eHmzZvWbfzwEzURkeM4qImIHMdBTUTkOA5qIiLHcVATETmOg5qIyHEc1EREjpMYD2pNSmVlpTY1NaV8v9Hy8vKM9fz8fGM9yHpUkwULFqC5udm8SDdKcXGxVlVVJXVM2zrnjo4OY72kpMRYf/v2bdw9DdXY2Hg36A3hRSTpN9/SpUuTqm/ZssVYP3r0qLWHzZs32zZJaybHjx831svLzQ/8Xrx4sbEe5Cb958+ft20SOBMgNbnU1NQY62vXrjXWX79+baxv2LDB2oNtXb+q+m7AT9RERI7joCYichwHNRGR4zioiYgcx0FNROQ4DmoiIsdxUBMROS6U+1Hb2NY4A8Dp06eN9ZUrVxrryd7POt1M92X+bP369cZ6WVmZsb5mzRpj/ciRI9Ye0uns2bPWbYqKiox129rwnJwcY/3WrVvWHtJp586d1m1mz55trO/atctYr6ysNNZta4EzYdWqVdZtbP9dO3bsMNbXrVtnrJ84ccLaQ6L4iZqIyHEc1EREjuOgJiJyHAc1EZHjOKiJiBzHQU1E5DgOaiIix2VkHXVPT491G9s6adv62cePH8fVU6Zt27bNus24ceOM9bq6OmN90aJFxvry5cutPaRTbW2tdRvbPbgXLlxorFdXVxvrly5dsvaQTg0NDdZtbGutt2/fbqwPDAwY6/X19dYe0u3cuXNJb1NQUGCsjxo1ylg/c+aMtYdEBRrUIvICwDsAAwD647nh9+/Vw4cPISL3wUyGms1cPJiJFzOJQzyfqBeqqvkRB9mHmfhjLl7MxIuZBMRz1EREjgs6qBXAP0Xkroj4njgUkVoRuSMid968eZO6Dh01eL+DwJn09vamt8HMiplLdCaZaCyDmIlX4O+fdDfmmqCnPr5V1VYRKQPwLxH5RVWvRm+gqscAHAM+Pdw2xX06Z/r06Xjw4MHcoJkUFxf/7jMZ9IuqxswlOpNUPLB0mGAmXsZMgKzNxVegT9Sq2jr4axuAegDzw2xqOIhEIgCYiY8+gLkMwUy8mEkcrINaREaJyDefvwbwZwAPwm7MZV1dXV+WMDGTr/r7+4HB9xRz+Q1m4sVM4hDk1Md4APWD52RHAjitqpdD7cpxbW1tePLkCUTkZzCTLwbPw89kLh7MxIuZxME6qFX1GYA/paGXuHR2dhrrFy9eDO3YU6dOxcyZM9Hc3JyyXIJcBGRju7gj7AtaCgsLAeBRqtbE2i5mCaKmpsZY37dvn7He3t6edA9IYSb37t1Leh9Xr1411ltaWpI+RgApyyRVuru7jfXGxkZj/fLl8P6u4fI8IiLHcVATETmOg5qIyHEc1EREjuOgJiJyHAc1EZHjOKiJiBwnqqm/hF5E2gG8jHppLADXb2cYb49/UFXznfyjZEkmQBy5MBMvn0wSPWa68fvHK2WZhDKoPQcRuePa4vah0t0jM8n88RKRiR6ZS+aPl4hU9shTH0REjuOgJiJyXLoG9bE0HScZ6e6RmWT+eInIRI/MJfPHS0TKekzLOWoiIkocT30QETku1EEtIktF5N8i0iIifwnzWMkQkRcicl9Efgr7+WzMJObxnM+FmXgxE38pz0VVQ/kHQA6ApwCmAcgF8DOAP4Z1vCR7fQFgbBqOw0yGcS7MhJlkKpcwP1HPB9Ciqs9UtRfADwC+C/F4wwEz8cdcvJiJV9ZmEuagLgfwa9TvXw2+5iKF5dH1KcJM/A2XXJiJFzPxl9JcgjwzMVHi85qrS0y+VdVW06PrU4SZ+BsuuTATL2biL6W5hPmJ+hWAKVG/nwygNcTjJUxVWwd/DfvR9czE37DIhZl4MRN/qc4lzEF9G8B0EZkqIrkAvgfQEOLxEiIio0Tkm89fI9xH1zMTf87nwky8mIm/MHIJ7dSHqvaLyBYAP+LT/639h6o+DOt4SRgPoF5EgJAfXc9M/A2TXJiJFzPxl/JceGUiEZHjeGUiEZHjOKiJiBzHQU1E5DgOaiIix3FQExE5joOaiMhxHNRERI7joCYictz/AdDUccSbpP87AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### 다시!! max pooling을 적용해 보자 \n",
    "pool = tf.nn.max_pool(conv2d, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "print(pool)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pool_img = pool.eval()\n",
    "\n",
    "#### 앞서 CNN을 거치면서 conv2d의 크기가 절반으로 줄어 14x14가 되었다.  \n",
    "#### Zero padding을 사용했지만 strides = 2이므로, 결과이미지 크기는 절반이 된다.(7x7)\n",
    "pool_img = np.swapaxes(pool_img, 0, 3)\n",
    "for i, one_img in enumerate (pool_img): \n",
    "    plt.subplot(1,5,i+1), plt.imshow(one_img.reshape(7,7), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 11-2: MNIST 99% with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000002102B2DA1D0>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000002102B2E2C50>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000002102B2DA400>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### tf.get_variable은 이전에 사용했던 변수명을 기억한다. 아래 코드로 이를 초기화 \n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-12-11a31463af24>:37: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Learning Started, It makes sometimes.\n",
      "Epoch: 1 cost =  0.33304703877392144\n",
      "Epoch: 2 cost =  0.08886961246755985\n",
      "Epoch: 3 cost =  0.06540637931617142\n",
      "Epoch: 4 cost =  0.05338967634576627\n",
      "Epoch: 5 cost =  0.047837416013618086\n",
      "Epoch: 6 cost =  0.04072400415972382\n",
      "Epoch: 7 cost =  0.03607198116411875\n",
      "Epoch: 8 cost =  0.031376901979092514\n",
      "Epoch: 9 cost =  0.02777647290093062\n",
      "Epoch: 10 cost =  0.026408656040068968\n",
      "Epoch: 11 cost =  0.022858931507110925\n",
      "Epoch: 12 cost =  0.019897843296904177\n",
      "Epoch: 13 cost =  0.01686150367329405\n",
      "Epoch: 14 cost =  0.016290754707456067\n",
      "Epoch: 15 cost =  0.014214352335045328\n",
      "Learning Finished\n",
      "Accuracy: 0.9855\n"
     ]
    }
   ],
   "source": [
    "#### 먼저 input image를 적절하게 지정해 준다\n",
    "#### MNIST는 28x28 사이즈의 흑백 이미지 784개로 이루어져 있으므로 ... \n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28,28,1])            \n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#### 1st layer \n",
    "#### 3x3x1의 필터를 32개 사용한 CNN을 거치고, 이를 다시 필터 2x2 & strides = 2로 sampleing(max_pooling)\n",
    "#### 따라서 결과는 절반으로 줄어들어 14x14가 된다. \n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev = 0.01))\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "#### 2nd layer \n",
    "#### layer 1의 결과를 다시 cnn에 투입, 필터는 3x3을 64개 사용 \n",
    "#### 이를 다시 필터 2x2 & strides = 2로 sampleing(max_pooling) \n",
    "#### 따라서 결과는 절반으로 줄어들어 7x7이 된다. \n",
    "W2 = tf.Variable(tf.random_normal([3,3,32,64], stddev = 0.01))\n",
    "L2 = tf.nn.conv2d(L1, W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "#### Fully Connected에 넣기위해 값들을 펼친다. 7x7 사이즈의 이미지가 64개 생성되었으므로, 7x7x64 = 3136개의 결과 이미지\n",
    "#### 최종 출력값은 0~9이므로 10이 된다. \n",
    "#### hyper parameters, 초기값 설정방법 지정, 가설과 cost 함수 설정, training 방법 설정 \n",
    "L2_flat = tf.reshape(L2, [-1, 7*7*64])\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "W3 = tf.get_variable('W3', shape = [7*7*64, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2_flat, W3) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "#### initialize \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#### training \n",
    "print('Learning Started, It makes sometimes.')\n",
    "\n",
    "for epoch in range(training_epochs) : \n",
    "    avg_cost = 0 \n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for i in range(total_batch) : \n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _, = sess.run([cost, optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print('Epoch:', epoch+1, 'cost = ', avg_cost)\n",
    "\n",
    "print('Learning Finished')\n",
    "\n",
    "#### test \n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict = {X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정확도 향상을 위해 layer를 보다 많이 쌓는 경우도 위와 같은 코드를 반복하는 과정\n",
    "- 단 drop out(keep_prob)의 설정에 주의, 여기서는 training = 0.7 & test = 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-331f8b9ce4c8>:23: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost = 0.405479272\n",
      "Epoch: 0002 cost = 0.098291552\n",
      "Epoch: 0003 cost = 0.074284309\n",
      "Epoch: 0004 cost = 0.061162627\n",
      "Epoch: 0005 cost = 0.052200512\n",
      "Epoch: 0006 cost = 0.048186214\n",
      "Epoch: 0007 cost = 0.042460790\n",
      "Epoch: 0008 cost = 0.039284753\n",
      "Epoch: 0009 cost = 0.035403663\n",
      "Epoch: 0010 cost = 0.035181244\n",
      "Epoch: 0011 cost = 0.033496085\n",
      "Epoch: 0012 cost = 0.030472410\n",
      "Epoch: 0013 cost = 0.029917258\n",
      "Epoch: 0014 cost = 0.029276439\n",
      "Epoch: 0015 cost = 0.027137286\n",
      "Learning Finished!\n",
      "Accuracy: 0.9939\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "# L3 ImgIn shape=(?, 7, 7, 64)\n",
    "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
    "                    1, 2, 2, 1], padding='SAME')\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
    "\n",
    "\n",
    "# L4 FC 4x4x128 inputs -> 625 outputs\n",
    "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([625]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "# L5 Final FC 625 inputs -> 10 outputs\n",
    "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L4, W5) + b5\n",
    "\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "# if you have a OOM error, please refer to lab-11-X-mnist_deep_cnn_low_memory.py\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "       X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 첫번째 2 layers CNN : Epoch 15회, Accuracy 0.9897 \n",
    "- 두번재 4 layers CNN : Epoch 15회, Accuracy 0.9932"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 11-3: CNN class, layer, ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class와 Layer를 사용해서, 복잡한 코드를 단순화 \n",
    "- 7개 모델을 ensemble하여 예측결과를 얻는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-98ed9409e9ee>:29: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-98ed9409e9ee>:32: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-98ed9409e9ee>:34: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-98ed9409e9ee>:55: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "Learning Started!\n",
      "Epoch: 0001 cost = [0.27946609 0.29032681]\n",
      "Epoch: 0002 cost = [0.08662839 0.08849391]\n",
      "Epoch: 0003 cost = [0.06624603 0.0659832 ]\n",
      "Epoch: 0004 cost = [0.05688636 0.05652943]\n",
      "Epoch: 0005 cost = [0.04583257 0.04818482]\n",
      "Epoch: 0006 cost = [0.04168963 0.04336172]\n",
      "Epoch: 0007 cost = [0.04026004 0.04128004]\n",
      "Epoch: 0008 cost = [0.03610833 0.03585229]\n",
      "Epoch: 0009 cost = [0.03544369 0.03459216]\n",
      "Epoch: 0010 cost = [0.03054557 0.03273134]\n",
      "Epoch: 0011 cost = [0.03130719 0.03169711]\n",
      "Epoch: 0012 cost = [0.03029879 0.02957265]\n",
      "Epoch: 0013 cost = [0.02816108 0.02877955]\n",
      "Epoch: 0014 cost = [0.0276086  0.02664824]\n",
      "Epoch: 0015 cost = [0.02498096 0.02416962]\n",
      "Epoch: 0016 cost = [0.02686235 0.02667604]\n",
      "Epoch: 0017 cost = [0.02428052 0.02364356]\n",
      "Epoch: 0018 cost = [0.02510866 0.02463901]\n",
      "Epoch: 0019 cost = [0.02148023 0.02564392]\n",
      "Epoch: 0020 cost = [0.02011296 0.02274787]\n",
      "Learning Finished!\n",
      "0 Accuracy: 0.9943\n",
      "1 Accuracy: 0.9947\n",
      "Ensemble accuracy: 0.9951\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
    "            # for testing\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "\n",
    "            # input place holders\n",
    "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "            # img 28x28x1 (black/white), Input Layer\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "            # Convolutional Layer #1\n",
    "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            # Pooling Layer #1\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout1 = tf.layers.dropout(inputs=pool1,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Convolutional Layer #2 and Pooling Layer #2\n",
    "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout2 = tf.layers.dropout(inputs=pool2,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Convolutional Layer #3 and Pooling Layer #3\n",
    "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout3 = tf.layers.dropout(inputs=pool3,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Dense Layer with Relu\n",
    "            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n",
    "            dense4 = tf.layers.dense(inputs=flat,\n",
    "                                     units=625, activation=tf.nn.relu)\n",
    "            dropout4 = tf.layers.dropout(inputs=dense4,\n",
    "                                         rate=0.5, training=self.training)\n",
    "\n",
    "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
    "            self.logits = tf.layers.dense(inputs=dropout4, units=10)\n",
    "\n",
    "        # define cost/loss & optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, training=False):\n",
    "        return self.sess.run(self.logits,\n",
    "                             feed_dict={self.X: x_test, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        return self.sess.run(self.accuracy,\n",
    "                             feed_dict={self.X: x_test,\n",
    "                                        self.Y: y_test, self.training: training})\n",
    "\n",
    "    def train(self, x_data, y_data, training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "            self.X: x_data, self.Y: y_data, self.training: training})\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "\n",
    "models = []\n",
    "num_models = 2\n",
    "for m in range(num_models):\n",
    "    models.append(Model(sess, \"model\" + str(m)))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Learning Started!')\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost_list = np.zeros(len(models))\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # train each model\n",
    "        for m_idx, m in enumerate(models):\n",
    "            c, _ = m.train(batch_xs, batch_ys)\n",
    "            avg_cost_list[m_idx] += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "test_size = len(mnist.test.labels)\n",
    "predictions = np.zeros([test_size, 10])\n",
    "for m_idx, m in enumerate(models):\n",
    "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
    "        mnist.test.images, mnist.test.labels))\n",
    "    p = m.predict(mnist.test.images)\n",
    "    predictions += p\n",
    "\n",
    "ensemble_correct_prediction = tf.equal(\n",
    "    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
    "ensemble_accuracy = tf.reduce_mean(\n",
    "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
    "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

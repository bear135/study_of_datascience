{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 10-1: ReLU (Better than sigmoid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 복습 : nn을 wide & deep 하게 만들기 (앞 단계의 출력값 = 다음단계의 입력값, bias갯수는 해당 단계의 출력값 갯수)\n",
    "\n",
    "<left><img src= 'img/three_layers.PNG' width=\"70%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 9개의 hidden layers를 가진 사례 (Tensorboard 표현을 위한 코드 추가)\n",
    "- input / output layer 각 1개 + hidden 9개 = 총 11개의 layers를 가진다. \n",
    "\n",
    "<left><img src= 'img/nine_layers.PNG' width=\"70%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'yellow'> Vanishing Gradient issue </font>\n",
    "- layer가 많아지면 오히려 그림과 같이 정확도를 얻기 힘들어 지는 현상 발생 \n",
    "- Sigmoid의 근본적인 약점에 기인한다. backpropagation에서 결과를 전달할 때 sigmoid를 사용한다. 그런데, sigmoid는 전달된 값을 0과 1 사이로 심하게 변형을 한다. 일정 비율로 줄어들기 때문에 왜곡은 아니지만, 값이 현저하게 작아지는 현상이 벌어진다. 3개의 layer를 거치면서 계속해서 1/10로 줄어들었다면, 현재 사용 중인 값은 처음 값의 1/1000이 된다. 이렇게 작아진 값을 갖고 원래의 영향력을 그대로 복원한다는 것은 불가능하다. (미분값이 0에 수렴) \n",
    "- backpropagation에서 전달하는 값은 똑같아 보이지만, layer를 지날 때마다 최초 값보다 현저하게 작아지기 때문에 값을 전달해도 의미를 가질 수가 없다.\n",
    "\n",
    "<left><img src= 'img/vanishing_g.PNG' width=\"40%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'yellow'> sigmoid 대신 relu를 쓰면 된다. </font>\n",
    "- sigmoid는 항상 1보다 작은수를 미분하기 때문에, 이러한 문제가 발생 \n",
    "- Relu는 0보다 작은 값은 버려버리고(0으로 처리), 0보다 큰 값은 그대로 사용\n",
    "\n",
    "<left><img src= 'img/lelu.PNG' width=\"70%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기존 코드에서 sigmoid를 relu로 바꿔주기만 하면 된다. \n",
    "\n",
    "<top><img src= 'img/relu1.PNG' width=\"30%\"></top>\n",
    "<left><img src= 'img/relu_cost.PNG' width=\"31.5%\"></left>\n",
    "\n",
    "<left><img src= 'img/relu2.PNG' width=\"40%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 10-2: weight를 초기화 잘하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nn의 초기값을 아무렇게나 주어서는 안된다. \n",
    "- 극단적으로 만약 모든 초기값을 0 (all weights = 0)으로 준다면, network는 전혀 작동하지 않을것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'yellow'> 초기화를 위한 방법 들 <font>\n",
    "- 1) RBM : (forward) 현재 layer에 들어온 x값에 대해 weight을 계산한 값을 다음 layer에 전달한다. (backward) 이렇게 전달 받은 값을 이번에는 거꾸로 이전(현재) layer에 weight 값을 계산해서 전달한다. forward와 backward 계산을 반복해서 진행하면, 최초 전달된 x와 예측한 값(x hat)의 차이가 최소가 되는 weight을 발견하게 된다. 앞에 설명한 내용을 2개 layer에 대해 초기값을 결정할 때까지 반복하고, 다음 번 2개에 대해 다시 반복하고. 이것을 마\n",
    "지막 layer까지 반복하는 방식이다. 이렇게 초기화된 모델을 Deep Belief Network라고 부른다. \n",
    "\n",
    "### <font color = 'yellow'> RBM 방식이 너무 복잡하다는 문제로 인해, 다음과 같은 방법들이 고안되었다. <font>\n",
    "- 2) xavier : 입력값과 출력값 사이의 난수를 선택해서 입력값의 제곱근으로 나눈다.\n",
    "- 3) he : 입력값을 반으로 나눈 제곱근 사용. 분모가 작아지기 때문에 xavier보다 넓은 범위의 난수 생성.\n",
    "    \n",
    "<left><img src= 'img/xavier_he.PNG' width=\"80%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 간단한 초기화 만으로도 성능이 비약적으로 향상된다. \n",
    "- 알고리즘 측면에서 sigmoid와 Relu를, 초기값 설정방법에서 xavier/he를 보다 개선시킬 수 있는 다양한 방법이 계속 연구되고 있다. \n",
    "\n",
    "<left><img src= 'img/scorings.PNG' width=\"60%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 10-3: NN Dropout and Model Ansemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'yellow'> overfitting을 방지하는 3가지 방법 </font>\n",
    "\n",
    "- 첫 번째 : training data를 많이 모으는 것이다. 데이터가 많으면 training set, validation set, test set으로 나누어서 진행할 수 있고, 영역별 데이터의 크기도 커지기 때문에 overfitting 확률이 낮아진다. 달리 생각하면 overfitting을 판단하기가 쉬워지기 때문에 정확한 모델을 만들 수 있다고 볼 수도 있다.\n",
    "\n",
    "- 두 번째 : feature의 갯수를 줄이는 것이다. 이들 문제는 multinomial classification에서도 다뤘었는데, 서로 비중이 다른 feature가 섞여서 weight에 대해 경합을 하면 오히려 좋지 않은 결과가 나왔었다. 그래서, feature 갯수를 줄이는 것이 중요한데, 이 부분은 deep learning에서는 중요하지 않다고 말씀하셨다. deep learning은 sigmoid 대신 LeRU 함수 계열을 사용하는 것도 있고 바로 뒤에 나오는 dropout을 통해 feature를 스스로 줄일 수 있는 방법도 있기 때문에.\n",
    "\n",
    "- 세 번째 : regularization. 앞에서는 weight이 너무 큰 값을 갖지 못하도록 제한하기 위한 방법으로 설명했다. 이 방법을 사용하면 weight이 커지지 않기 때문에 선이 구부러지는 형태를 피할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'yellow'> Regularization 기법 : 1. L2reg </font>\n",
    "- 람다값을 보통 0.001로 지정함. 중요도가 높다면 0.01 또는 0.1로 지정할 수 도 있음 \n",
    "- 0.001이 작아 보이지만 weight 제곱들에 모두 곱해져서 합산되므로, 꽤 큰 영향력을 미치게 된다. \n",
    "\n",
    "<left><img src= 'img/l2reg.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'yellow'> Regularization 기법 : 2. Dropout </font>\n",
    "- \"사공이 너무 많으면 배가 산으로 간다.\", layer에 포함된 일부 weights를 의도적으로 계산에서 배제하는 방법   \n",
    "- forward pass로 데이터를 전달할 때, 난수를 사용해서 일부 neuron을 0으로 만드는 방식으로 구현한다. \n",
    "- 단 dropout은 training 단계에서만 사용한다. test단계에서는 모든 weights를 사용해야 함을 주의하라!! \n",
    "\n",
    "<left><img src= 'img/dropout.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'yellow'> Regularization 기법 : 3. Ensemble </font>\n",
    "- 여러 전문가들에게서 동시에 자문을 받는 것과 비슷하다.  \n",
    "- 데이터를 여러 개의 training set으로 나누어서 동시에 학습을 진행, 모든 training set에 대한 학습이 끝나면 결과를 통합하는 방법\n",
    "\n",
    "<left><img src= 'img/ensemble.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 10-4: NN Lego play "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 딥러닝 모델을 설계하는 작업은, 마치 레고블럭을 어떤 형태로 쌓고 나열할 것인가와 비슷하다. (창의력!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 1. Feedforward Neural network </font>\n",
    "- 단순하게 필요한 만큼 블럭을 가져다 꽂는 방식 \n",
    "- 단 블럭수가 많아지면 연산시간 또한 급증, GPU가 동원되는 이유이기도 하다. \n",
    "\n",
    "<left><img src= 'img/feedforward.PNG' width=\"20%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 2. Fastforward Neural network </font>\n",
    "- 빨기감기!! 앞으로 이동할때 꼭 한칸만 갈 필요는 없다. 2칸 또는 3칸 앞으로도 갈 수 있다.\n",
    "- 2015년 홍콩중문대 he가 사용하여, Imagenet에서 error rate = 3% under를 거두면서 유명해졌다. \n",
    "\n",
    "<left><img src= 'img/fastforward.PNG' width=\"70%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 3. Convolutional Neural Network(CNN) </font>\n",
    "- 레이어가 중간에 벌어졌다가 나중에 다시 모이거나, 처음부터 여러 개로 출발해서 나중에 하나로 합쳐지는 형태도 가능 \n",
    "- TensorFlow에 포함된 mnist 모델이 CNN으로 구축되었고, 99.2%의 정확도를 보여줌\n",
    "\n",
    "<left><img src= 'img/cnn.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 4. Recurrent Neural Network(RNN) </font>\n",
    "- 앞으로, 옆으로 ... 여려방향으로도 진행 가능함 \n",
    "- 연속된 방식으로 입력되는 음성인식 등의 분야에서 활발하게 연구 중\n",
    "\n",
    "<left><img src= 'img/rnn.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 10: 딥러닝으로 MNIST 98%이상 해보기\n",
    "- Relu, Xavier, Dropout, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 실습을 위해 텐서플로우 버전 다운그레이드 \n",
    "#!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 1. Softmax 복습 (Lec 07) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-8feb2de57d3a>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Epoch: 0001 cost = 4.937201931\n",
      "Epoch: 0002 cost = 1.594202023\n",
      "Epoch: 0003 cost = 1.092172472\n",
      "Epoch: 0004 cost = 0.883611534\n",
      "Epoch: 0005 cost = 0.762665251\n",
      "Epoch: 0006 cost = 0.682012059\n",
      "Epoch: 0007 cost = 0.623473834\n",
      "Epoch: 0008 cost = 0.578316215\n",
      "Epoch: 0009 cost = 0.542602040\n",
      "Epoch: 0010 cost = 0.513874937\n",
      "Epoch: 0011 cost = 0.489708639\n",
      "Epoch: 0012 cost = 0.469853163\n",
      "Epoch: 0013 cost = 0.452419695\n",
      "Epoch: 0014 cost = 0.437093707\n",
      "Epoch: 0015 cost = 0.423534533\n",
      "Epoch: 0016 cost = 0.411639703\n",
      "Epoch: 0017 cost = 0.400659952\n",
      "Epoch: 0018 cost = 0.391476652\n",
      "Epoch: 0019 cost = 0.382535982\n",
      "Epoch: 0020 cost = 0.374324498\n",
      "Epoch: 0021 cost = 0.366999364\n",
      "Epoch: 0022 cost = 0.360261758\n",
      "Epoch: 0023 cost = 0.354018938\n",
      "Epoch: 0024 cost = 0.348362560\n",
      "Epoch: 0025 cost = 0.343254921\n",
      "Epoch: 0026 cost = 0.337857972\n",
      "Epoch: 0027 cost = 0.332845558\n",
      "Epoch: 0028 cost = 0.329018954\n",
      "Epoch: 0029 cost = 0.325055470\n",
      "Epoch: 0030 cost = 0.320971149\n",
      "Epoch: 0031 cost = 0.317426714\n",
      "Epoch: 0032 cost = 0.313942208\n",
      "Epoch: 0033 cost = 0.310806153\n",
      "Epoch: 0034 cost = 0.307944130\n",
      "Epoch: 0035 cost = 0.304233144\n",
      "Epoch: 0036 cost = 0.301878548\n",
      "Epoch: 0037 cost = 0.299241573\n",
      "Epoch: 0038 cost = 0.296669996\n",
      "Epoch: 0039 cost = 0.294366432\n",
      "Epoch: 0040 cost = 0.292207657\n",
      "Epoch: 0041 cost = 0.289871961\n",
      "Epoch: 0042 cost = 0.287350794\n",
      "Epoch: 0043 cost = 0.285437136\n",
      "Epoch: 0044 cost = 0.283673539\n",
      "Epoch: 0045 cost = 0.281995449\n",
      "Epoch: 0046 cost = 0.280459484\n",
      "Epoch: 0047 cost = 0.278301885\n",
      "Epoch: 0048 cost = 0.277012810\n",
      "Epoch: 0049 cost = 0.275339036\n",
      "Epoch: 0050 cost = 0.273723938\n",
      "Learning Finished!\n",
      "Accuracy: 0.9176\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "nb_classes = 10\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# train my model\n",
    "for epoch in range(num_epochs): \n",
    "    avg_cost = 0\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict={X: batch_xs, Y: batch_ys}\n",
    "        _, cost_val = sess.run([train, cost], feed_dict = feed_dict)\n",
    "        avg_cost += cost_val / num_iterations\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch+1), \n",
    "          'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print(\"Learning Finished!\")\n",
    "\n",
    "    \n",
    "    # Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "print(\"Accuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 2. Neural Net </font>\n",
    "- soft max 결과 정확도는 0.91, NN을 적용하여 이를 개선해 보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 167.705880907\n",
      "Epoch: 0002 cost = 39.807481595\n",
      "Epoch: 0003 cost = 24.986590509\n",
      "Epoch: 0004 cost = 17.471965670\n",
      "Epoch: 0005 cost = 12.771477773\n",
      "Epoch: 0006 cost = 9.460116684\n",
      "Epoch: 0007 cost = 7.020089166\n",
      "Epoch: 0008 cost = 5.274151364\n",
      "Epoch: 0009 cost = 3.829028910\n",
      "Epoch: 0010 cost = 2.926844186\n",
      "Epoch: 0011 cost = 2.228807511\n",
      "Epoch: 0012 cost = 1.667547674\n",
      "Epoch: 0013 cost = 1.277634554\n",
      "Epoch: 0014 cost = 0.960484041\n",
      "Epoch: 0015 cost = 0.769280848\n",
      "Epoch: 0016 cost = 0.685478824\n",
      "Epoch: 0017 cost = 0.552528870\n",
      "Epoch: 0018 cost = 0.503930909\n",
      "Epoch: 0019 cost = 0.467435413\n",
      "Epoch: 0020 cost = 0.435325658\n",
      "Epoch: 0021 cost = 0.381316225\n",
      "Epoch: 0022 cost = 0.277392108\n",
      "Epoch: 0023 cost = 0.361459498\n",
      "Epoch: 0024 cost = 0.400262642\n",
      "Epoch: 0025 cost = 0.285680667\n",
      "Epoch: 0026 cost = 0.295016600\n",
      "Epoch: 0027 cost = 0.355461390\n",
      "Epoch: 0028 cost = 0.279670528\n",
      "Epoch: 0029 cost = 0.251232306\n",
      "Epoch: 0030 cost = 0.266238436\n",
      "Epoch: 0031 cost = 0.242046412\n",
      "Epoch: 0032 cost = 0.245266037\n",
      "Epoch: 0033 cost = 0.228466949\n",
      "Epoch: 0034 cost = 0.207535822\n",
      "Epoch: 0035 cost = 0.172879688\n",
      "Epoch: 0036 cost = 0.191722621\n",
      "Epoch: 0037 cost = 0.239321469\n",
      "Epoch: 0038 cost = 0.192270321\n",
      "Epoch: 0039 cost = 0.231198719\n",
      "Epoch: 0040 cost = 0.181605011\n",
      "Epoch: 0041 cost = 0.197931213\n",
      "Epoch: 0042 cost = 0.158655146\n",
      "Epoch: 0043 cost = 0.235912312\n",
      "Epoch: 0044 cost = 0.145320818\n",
      "Epoch: 0045 cost = 0.112431143\n",
      "Epoch: 0046 cost = 0.188565051\n",
      "Epoch: 0047 cost = 0.211967855\n",
      "Epoch: 0048 cost = 0.168110039\n",
      "Epoch: 0049 cost = 0.148712983\n",
      "Epoch: 0050 cost = 0.199848052\n",
      "Learning Finished!\n",
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers (3 layers & Relu model)\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256])) \n",
    "L1 = tf.nn.relu(tf.matmul(X, W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256])) \n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10])) \n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# train my model\n",
    "for epoch in range(num_epochs): \n",
    "    avg_cost = 0\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict={X: batch_xs, Y: batch_ys}\n",
    "        _, cost_val = sess.run([train, cost], feed_dict = feed_dict)\n",
    "        avg_cost += cost_val / num_iterations\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch+1), \n",
    "          'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print(\"Learning Finished!\")\n",
    "\n",
    "    \n",
    "    # Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "print(\"Accuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 3. Xavier initialization </font>\n",
    "- 3 layers & Relu가 적용된 NN을 통해, 0.96의 정확도를 얻었다. 초기값 조정을 통해 좀더 개선시켜 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Epoch: 0001 cost = 0.319076312\n",
      "Epoch: 0002 cost = 0.113941745\n",
      "Epoch: 0003 cost = 0.076022439\n",
      "Epoch: 0004 cost = 0.052442706\n",
      "Epoch: 0005 cost = 0.039448058\n",
      "Epoch: 0006 cost = 0.027360335\n",
      "Epoch: 0007 cost = 0.023502130\n",
      "Epoch: 0008 cost = 0.019484432\n",
      "Epoch: 0009 cost = 0.015803255\n",
      "Epoch: 0010 cost = 0.013046877\n",
      "Epoch: 0011 cost = 0.011725356\n",
      "Epoch: 0012 cost = 0.014272317\n",
      "Epoch: 0013 cost = 0.007803231\n",
      "Epoch: 0014 cost = 0.010792380\n",
      "Epoch: 0015 cost = 0.009559013\n",
      "Epoch: 0016 cost = 0.007355750\n",
      "Epoch: 0017 cost = 0.009187714\n",
      "Epoch: 0018 cost = 0.008989583\n",
      "Epoch: 0019 cost = 0.005579319\n",
      "Epoch: 0020 cost = 0.007179190\n",
      "Epoch: 0021 cost = 0.011406213\n",
      "Epoch: 0022 cost = 0.006412306\n",
      "Epoch: 0023 cost = 0.002675119\n",
      "Epoch: 0024 cost = 0.008939071\n",
      "Epoch: 0025 cost = 0.007717719\n",
      "Epoch: 0026 cost = 0.002686251\n",
      "Epoch: 0027 cost = 0.003462042\n",
      "Epoch: 0028 cost = 0.009293941\n",
      "Epoch: 0029 cost = 0.003150761\n",
      "Epoch: 0030 cost = 0.005253764\n",
      "Epoch: 0031 cost = 0.007506283\n",
      "Epoch: 0032 cost = 0.002929432\n",
      "Epoch: 0033 cost = 0.004110928\n",
      "Epoch: 0034 cost = 0.007532167\n",
      "Epoch: 0035 cost = 0.004905439\n",
      "Epoch: 0036 cost = 0.003837526\n",
      "Epoch: 0037 cost = 0.002927214\n",
      "Epoch: 0038 cost = 0.006475471\n",
      "Epoch: 0039 cost = 0.006911925\n",
      "Epoch: 0040 cost = 0.004590714\n",
      "Epoch: 0041 cost = 0.001833816\n",
      "Epoch: 0042 cost = 0.004547027\n",
      "Epoch: 0043 cost = 0.003246914\n",
      "Epoch: 0044 cost = 0.004261122\n",
      "Epoch: 0045 cost = 0.004628882\n",
      "Epoch: 0046 cost = 0.004125801\n",
      "Epoch: 0047 cost = 0.002412869\n",
      "Epoch: 0048 cost = 0.004980309\n",
      "Epoch: 0049 cost = 0.004840681\n",
      "Epoch: 0050 cost = 0.005354042\n",
      "Learning Finished!\n",
      "Accuracy: 0.9815\n"
     ]
    }
   ],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers (3 layers & Relu & Xavier initialization model)\n",
    "W1 = tf.get_variable('W1', shape = [784, 256], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer() )\n",
    "b1 = tf.Variable(tf.random_normal([256])) \n",
    "L1 = tf.nn.relu(tf.matmul(X, W1)+b1)\n",
    "\n",
    "W2 = tf.get_variable('W2', shape = [256, 256], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer() )\n",
    "b2 = tf.Variable(tf.random_normal([256])) \n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2)+b2)\n",
    "\n",
    "W3 = tf.get_variable('W3', shape = [256, 10], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer() )\n",
    "b3 = tf.Variable(tf.random_normal([10])) \n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# train my model\n",
    "for epoch in range(num_epochs): \n",
    "    avg_cost = 0\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict={X: batch_xs, Y: batch_ys}\n",
    "        _, cost_val = sess.run([train, cost], feed_dict = feed_dict)\n",
    "        avg_cost += cost_val / num_iterations\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch+1), \n",
    "          'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print(\"Learning Finished!\")\n",
    "\n",
    "    \n",
    "    # Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "print(\"Accuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 4. Deep & Wide NN (with Drop out) </font>\n",
    "- 3 layers & Relu & xavier를 통해, 0.98의 정확도를 얻었다. 모델을 보다 Deep & wide하게 만들어 보자 \n",
    "- Overfitting의 우려가 있으니 Drop out도 적용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### tf.get_variable은 이전에 사용했던 변수명을 기억한다. \n",
    "#### 아래 코드로 이를 초기화 한다. \n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-15-993687e2dbd5>:21: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-15-993687e2dbd5>:48: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 cost = 0.441573110\n",
      "Epoch: 0002 cost = 0.164331779\n",
      "Epoch: 0003 cost = 0.129511973\n",
      "Epoch: 0004 cost = 0.107445169\n",
      "Epoch: 0005 cost = 0.090566491\n",
      "Epoch: 0006 cost = 0.082764879\n",
      "Epoch: 0007 cost = 0.074416832\n",
      "Epoch: 0008 cost = 0.065799660\n",
      "Epoch: 0009 cost = 0.063939734\n",
      "Epoch: 0010 cost = 0.058692063\n",
      "Epoch: 0011 cost = 0.055666615\n",
      "Epoch: 0012 cost = 0.051728384\n",
      "Epoch: 0013 cost = 0.047040064\n",
      "Epoch: 0014 cost = 0.044048360\n",
      "Epoch: 0015 cost = 0.047030777\n",
      "Learning Finished!\n",
      "Accuracy: 0.9817\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = 'yellow'> 참고. Adam optimizer </font>\n",
    "- adam의 성능이 상당이 뛰어나고, 사용법도 간편하다.(GradientDescentOptimizer -> AdamOptimizer로 대체)\n",
    "##### optimizer = tf.train.<font color = 'yellow'>GradientDescentOptimizer</font>(learning_rate = 0.01).minimize(cost) \n",
    "##### --> optimizer = tf.train.<font color = 'yellow'>AdamOptimizer</font>(learning_rate = 0.01).minimize(cost)\n",
    "  \n",
    "<left><img src= 'img/adam.PNG' width=\"40%\"></left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 09-1: XOR문제 딥러닝으로 풀기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - cf. (복습) sigmoid 함수 : x값이 커질수록 1에, 작아질수록 0에 근접한 값을 가진다.\n",
    " \n",
    " <left><img src= 'img/sigmoid.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logistic regression 한 개만 사용해서는 XOR 문제를 해결할 수 없지만, 그림처럼 3개를 연결해서 사용한다면 가능하다. \n",
    "- 이 부분이 Neural Network의 핵심일 수 있다. 기존의 방식을 연결해서 사용! \n",
    "\n",
    "<left><img src= 'img/xor_2.PNG' width=\"40%\"></left>\n",
    "<left><img src= 'img/xor_3.PNG' width=\"31%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 3개의 logistic을 합쳐서 Neural Network(NN)을 구성할 수 있다. \n",
    "- 이것을 다시 matrix를 적용하여 우측 그림과 같이 간단히 표현할 수 있다. \n",
    "\n",
    "<left><img src= 'img/nn.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 코딩으로 표현하면 아래와 같다.  \n",
    "- 마지막 수식이 k가 되고, 여기에 다시 sigmoid를 적용한 함수가 가설 H(x)가 된다. \n",
    "\n",
    "<left><img src= 'img/nn2.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 09-x: 10분안에 미분 정리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미분은 순간변화량, 즉 기울기를 구하는 것이다.\n",
    "- <font color = 'yellow'> 상수(constant value)에 대한 미분값은 항상 0이다. </font>\n",
    "\n",
    "<left><img src= 'img/derivative.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 편미분은 두 변수 중 하나에 대해서만 미분하고, 나머지 변수는 상수로 취급한다는 것이다. \n",
    "\n",
    "<left><img src= 'img/p_derivative.PNG' width=\"31%\"></left>\n",
    "<left><img src= 'img/p_derivative2.PNG' width=\"30%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 09-2: 딥넷트웍 학습 시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural Network의 node가 증가함에 따라 미분계산량이 급증, 현실적으로 계산해내기 어려웠다. \n",
    "- 계산결과를 실제값과 비교하여, 이를 다시 Backward Propagation하여 W를 재조정 하는 방식이 고안되었다. \n",
    "\n",
    "<left><img src= 'img/back_propagation.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward 값은 training data로 부터 가져온 실제 값이다. \n",
    "- 편미분을 통해 w, x, b의 미분값(기울기, 즉 변화량이 결과값에 미치는 영향도)을 구할 수 있다. \n",
    "- <font color = 'yellow'> 이처럼 알고있는 값으로 부터 시작해 거꾸로 미분을 수행하는 것이 backword propagation이다. </font>\n",
    "\n",
    "<left><img src= 'img/chain_rule.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 같은 방식을 여러개의 노드에 반복함으로써, Neural Network의 계산이 이루어진다. \n",
    "\n",
    "<left><img src= 'img/chain_rule2.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 09-1: Neural net for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\youngboo.choi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#### xor 문제를 tf로 풀어보기 \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "#### XOR 표를 데이터 형태로 만들어 읽어온다. \n",
    "x_data = np.array([ [0,0], [0,1], [1,0], [1,1] ], dtype = np.float32)\n",
    "y_data = np.array([ [0],   [1],   [1],   [0] ],   dtype = np.float32)\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### logistic으로 xor을 풀어보면 어떨까? \n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([2,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "#### hypothesis > 0.5 => True \n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9006585 [[0.9673923]\n",
      " [2.4748657]]\n",
      "\n",
      " >Hypothesis : [[0.11898362]\n",
      " [0.6160404 ]\n",
      " [0.26217428]\n",
      " [0.8084806 ]] \n",
      " >Correct : [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]] \n",
      " >Accuracy : 0.5\n",
      "2000 0.69314724 [[0.00074925]\n",
      " [0.00075491]]\n",
      "\n",
      " >Hypothesis : [[0.499777  ]\n",
      " [0.49996567]\n",
      " [0.4999643 ]\n",
      " [0.500153  ]] \n",
      " >Correct : [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]] \n",
      " >Accuracy : 0.25\n",
      "4000 0.6931472 [[2.6708702e-07]\n",
      " [2.6609246e-07]]\n",
      "\n",
      " >Hypothesis : [[0.49999988]\n",
      " [0.5       ]\n",
      " [0.5       ]\n",
      " [0.50000006]] \n",
      " >Correct : [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]] \n",
      " >Accuracy : 0.25\n",
      "6000 0.6931472 [[1.3297606e-07]\n",
      " [1.3198149e-07]]\n",
      "\n",
      " >Hypothesis : [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      " >Correct : [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      " >Accuracy : 0.5\n",
      "8000 0.6931472 [[1.3297606e-07]\n",
      " [1.3198149e-07]]\n",
      "\n",
      " >Hypothesis : [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      " >Correct : [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      " >Accuracy : 0.5\n",
      "10000 0.6931472 [[1.3297606e-07]\n",
      " [1.3198149e-07]]\n",
      "\n",
      " >Hypothesis : [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      " >Correct : [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      " >Accuracy : 0.5\n"
     ]
    }
   ],
   "source": [
    "#### graphing \n",
    "with tf.Session() as sess : \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001) : \n",
    "        sess.run(train, feed_dict = {X: x_data, Y: y_data})\n",
    "        if step %2000 == 0 : \n",
    "            print(step, sess.run(cost, feed_dict = {X: x_data, Y: y_data}), sess.run(W))\n",
    "            #### accuracy report \n",
    "            h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "            print('\\n >Hypothesis :', h, \n",
    "                  '\\n >Correct :', c, \n",
    "                  '\\n >Accuracy :', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XOR 문제에 대해 logistic의 정확도는 0.5밖에 나오지 않았다\n",
    "- XOR 문제를 풀기 위해서는 Neural Network가 필요하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color = 'yellow'> 두개의 layer를 구성하면 된다. </font>\n",
    "\n",
    "<left><img src= 'img/nn.PNG' width=\"50%\"></left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.93360746 [[0.41870815]\n",
      " [0.6592225 ]]\n",
      "\n",
      " >Hypothesis : [[0.10637787 0.32830006]\n",
      " [0.14445806 0.4094247 ]\n",
      " [0.10688201 0.32946822]\n",
      " [0.16724578 0.45193085]] \n",
      " >Correct : [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]] \n",
      " >Accuracy : 0.5\n",
      "2000 0.44096738 [[0.41870815]\n",
      " [0.6592225 ]]\n",
      "\n",
      " >Hypothesis : [[0.12406418 0.12406418]\n",
      " [0.6243787  0.6243787 ]\n",
      " [0.64046335 0.64046335]\n",
      " [0.51073223 0.51073223]] \n",
      " >Correct : [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]] \n",
      " >Accuracy : 0.75\n",
      "4000 0.081854045 [[0.41870815]\n",
      " [0.6592225 ]]\n",
      "\n",
      " >Hypothesis : [[0.04733804 0.04733804]\n",
      " [0.9105731  0.9105731 ]\n",
      " [0.9102638  0.9102638 ]\n",
      " [0.08718234 0.08718234]] \n",
      " >Correct : [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      " >Accuracy : 1.0\n",
      "6000 0.03514751 [[0.41870815]\n",
      " [0.6592225 ]]\n",
      "\n",
      " >Hypothesis : [[0.02393755 0.02393755]\n",
      " [0.96012276 0.96012276]\n",
      " [0.96002114 0.96002114]\n",
      " [0.03426635 0.03426635]] \n",
      " >Correct : [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      " >Accuracy : 1.0\n",
      "8000 0.021771602 [[0.41870815]\n",
      " [0.6592225 ]]\n",
      "\n",
      " >Hypothesis : [[0.01562652 0.01562652]\n",
      " [0.97498906 0.97498906]\n",
      " [0.97493887 0.97493887]\n",
      " [0.02041563 0.02041563]] \n",
      " >Correct : [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      " >Accuracy : 1.0\n",
      "10000 0.015639592 [[0.41870815]\n",
      " [0.6592225 ]]\n",
      "\n",
      " >Hypothesis : [[0.01151329 0.01151329]\n",
      " [0.9819176  0.9819176 ]\n",
      " [0.9818872  0.9818872 ]\n",
      " [0.01434767 0.01434767]] \n",
      " >Correct : [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      " >Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([ [0,0], [0,1], [1,0], [1,1] ], dtype = np.float32)\n",
    "y_data = np.array([ [0],   [1],   [1],   [0] ],   dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "#### 두개의 layer를 가지는 nn을 구성하고, 이로부터 가설을 만든다 #######################\n",
    "#### layer1은 출력값이 2개임에 주의!!\n",
    "W1 = tf.Variable(tf.random_normal([2,2]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2,1]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([2]), name = 'bias2')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "########################################################################################\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "#### hypothesis > 0.5 => True \n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "#### graphing \n",
    "with tf.Session() as sess : \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001) : \n",
    "        sess.run(train, feed_dict = {X: x_data, Y: y_data})\n",
    "        if step %2000 == 0 : \n",
    "            print(step, sess.run(cost, feed_dict = {X: x_data, Y: y_data}), sess.run(W))\n",
    "            #### accuracy report \n",
    "            h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "            print('\\n >Hypothesis :', h, \n",
    "                  '\\n >Correct :', c, \n",
    "                  '\\n >Accuracy :', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NN 결과 100%의 정확도로 xor 문제를 풀어내었다. \n",
    "- <font color = 'yellow'> 처음 입력값의 갯수(여기서는 2)와 마지막 출력값의 갯수(여기서는 1)만 틀리지 않으면, 모형을 얼마든지 wide & deep 하게 만들수 있다. \n",
    "  (앞단계의 출력값 = 그 다음단계의 입력값)</font>\n",
    "\n",
    "<left><img src= 'img/deepnn.PNG' width=\"55%\"></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 09-2: Tensor board "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. 텐서플로우에 loging할 값을 결정한다. \n",
    "- 2. 값들을 모두 합친다. \n",
    "- 3. 합쳐진 값을 저장할 위치를 정하고, 세션에 그래프를 넣는다. \n",
    "- 4. 세션을 실행하고, 파일에 기록한다.  \n",
    "- 5. tensorboard를 실행하고, 기록된 파일의 위치를 알려준다. \n",
    "\n",
    "<left><img src= 'img/five_steps.PNG' width=\"45%\"></left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7323932\n",
      "2000 0.34923428\n",
      "4000 0.3471486\n",
      "6000 0.34675288\n",
      "8000 0.3466356\n",
      "10000 0.34659585\n",
      "\n",
      " >Hypothesis : [[5.4453267e-05]\n",
      " [9.9997973e-01]\n",
      " [4.9999404e-01]\n",
      " [5.0000119e-01]] \n",
      " >Correct : [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]] \n",
      " >Accuracy : 0.5\n"
     ]
    }
   ],
   "source": [
    "#### MNIST를 tensorboard에 적용해 보자 (tf 2.0 code)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"Layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2]), name=\"weight_1\")\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name=\"bias_1\")\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    tf.summary.histogram(\"W1\", W1)\n",
    "    tf.summary.histogram(\"b1\", b1)\n",
    "    tf.summary.histogram(\"Layer1\", layer1)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name=\"weight_2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name=\"bias_2\")\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "    tf.summary.histogram(\"W2\", W2)\n",
    "    tf.summary.histogram(\"b2\", b2)\n",
    "    tf.summary.histogram(\"Hypothesis\", hypothesis)\n",
    "\n",
    "# cost/loss function\n",
    "with tf.name_scope(\"Cost\"):\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "    tf.summary.scalar(\"Cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"Train\"):\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_logs\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, summary, cost_val = sess.run(\n",
    "            [train, merged_summary, cost], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, p, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    \n",
    "    #print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")\n",
    "    \n",
    "    print('\\n >Hypothesis :', h, \n",
    "          '\\n >Correct :', p, \n",
    "          '\\n >Accuracy :', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='yellow' > Tensorboard 실행하기 </font>\n",
    "- 1) 주피터, 아나콘다 종료 후 다시 아나콘다 cmd 실행 \n",
    "- 2) 로그가 저장된 \"D:\\tensor_logs\" 폴더로 이동\n",
    "- 3) tensorboard --logdir=./ 실행 \n",
    "- 4) 웹브라우저 주소창에 http://localhost:6006 입력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
